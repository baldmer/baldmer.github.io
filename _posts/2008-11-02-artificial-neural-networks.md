---
layout: post
title: Artificial Neural Networks Theory
tags: [Data-Science, Neural-Networks]
---

Neural Networks has been an interesting topic for me since I was studying my bachelor. The basic idea about Neural Networks is inspired by the study of human neural processing. In terms of computer programming it is just a simple data structure in which nodes are interconnected.

These are some concepts:

#### Weights

The weights are used on the connections between different layers.

#### Training

Steps:

Start the neural network with one set of weights.
Run the network.
Modify some of all the weights.
Run the network again.

Repeat these steps until some predetermined goal is met. This process is called training.

#### No Training

Start the network with one set of weights and do not modify any of them.

#### Memory

There are two approaches to this concept:

Long-term memory (LTM) is memory associated with learning that persists for the long term
Short-term memory (STM) is memory associated with a neural network that decays in some time interval.

#### Supervised Learning

The learning would be supervised if external criteria are used and matched by the network output. You apply the inputs to the supervised network along with an expected response.

#### Unsupervised Learning / self-organizing

There are only inputs. There is more interaction between neurons, typically with feedback and intra-layer connections between neurons.

#### Binary and Bipolar Inputs

There are two types of inputs used in neural networks, Binary and Bipolar. Binary inputs have one of two values, 0 and 1; and Bipolar inputs have, 1 and â€“1; -1 of bipolar correspond to a 0 of binary.

#### Noise

Is perturbation in the data used to train the neural network. For example, an image may have random speckles in it.

